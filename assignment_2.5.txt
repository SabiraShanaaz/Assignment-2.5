1. Explain what is a cluster and what is a hadoop cluster

  *Cluster:
        -Cluster is a group of servers or other resources that are connected together to act like a single system.

        -Cluster provide high availability,load balancing and parallel processing.

        -Cluster acts as a  logical unit of file storage on hard disk.

        -The clusters are managed by operating systems.

        -The size of the cluster can be varied depending on the size of the data.

        -Clusters perform multiple complex instructions by distributing workload across all the connected servers.

  *Hadoop cluster:
        -It is a type of computational cluster designed for storing and analyzing unstructured data in distributed environment.
 
        -Cluster is a set of connected computers that work together as a single system.

        -The computer cluster used for hadoop is called as hadoop cluster.

        -The hadoop cluster run on low cost commodity computers.

        -Namenode and datanode are used to check the status of clusters.

        -The cluster is made up of racks and each rack contains namenode,job tracker,secondary namenode etc.
        
        -Hadoop cluster speed up the processing of data analysis.


2. Explain the components of Hadoop 1.x


  -The components of hadoop 1.x are:
                
   1)HDFS: Hadoop Distributed File System take the data breaks them into pieces 
            and distributes them to different nodes in a cluster and thus allows parallel processing.
     
     -The two components of HDFS are:
           -Namenode
           -Datanode

     *Namenode:
              -The namenode acts as the master node.
              -It is used to store metadata.
              -It stores the information about the data nodes like
                   how many blocks are stored in the nodes,which node holds the data,node locations etc.

     *Secondary namenode:
              -secondary namenodes performs house-keeping activities for name node like periodic merging and it is used for edits.

     *Datanode:
              -The datanode acts as slave nodes.

              -The data nodes are used to store the actual data.

              -It stores the data in terms of blocks.The default size of the block is 64MB.

              -The data node acts as the heart beat of the name node as it periodically sends signals to the name node
                   to check whether it is active.

    2)Map Reduce: Map reduce is also called as distributed data processing or batch processing model 
                         because it distributes the data across several machines.

        -The two components of map reduce are:
                -Job tracker.
                -Task tracker.

        *Job Tracker:
               -The main role of job tracker is to assign the map reduce task to the task tracker.

               -It is also used to re-assign some task to the other task tracker
                         when the previous task trackers fails or when it gets shut down.

               -Job tracker also maintains the status of the task tracker.

               -It is used to control the overall execution of map reduce.

        *Task Tracker:
               -The role of the task tracker is to execute the task assigned to it by the job tracker.

               -The task tracker sends the status back to the job tracker.

               -It runs individual map-reduce tasks on the data nodes.

               -It periodically communicates with the task tracker.

